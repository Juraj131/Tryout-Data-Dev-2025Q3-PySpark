{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Account Balance Report\n",
    "\n",
    "## Task Description\n",
    "Create a report with the following information:\n",
    "- Account number\n",
    "- Customer  \n",
    "- Opening balance 2025\n",
    "- Balance end of January, February, March, ... through December\n",
    "- Sum of transactions YTD (Year-to-Date)\n",
    "\n",
    "## Data Sources\n",
    "1. **Account Entries** - transaction data (AccountEntries.csv)\n",
    "2. **Account Information** - account details (AccountInformation.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries successfully imported!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"All libraries successfully imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d441792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Environment Diagnostics:\n",
      "Python executable: c:\\Users\\juraj\\Desktop\\ucoudify_pyspark\\pyspark_env_final\\Scripts\\python.exe\n",
      "Python version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "Current working directory: c:\\Users\\juraj\\Desktop\\ucoudify_pyspark\\Tryout-Data-Dev-2025Q3-PySpark\\Data\n",
      "\n",
      "ðŸ” Java check:\n",
      "âœ… Java is available\n",
      "openjdk version \"11.0.28\" 2025-07-15\n",
      "OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6)\n",
      "OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode)\n",
      "\n",
      "\n",
      "ðŸ” PySpark check:\n",
      "âœ… PySpark version: 3.5.1\n",
      "PySpark location: c:\\Users\\juraj\\Desktop\\ucoudify_pyspark\\pyspark_env_final\\Lib\\site-packages\\pyspark\\__init__.py\n",
      "\n",
      "ðŸ” Environment variables:\n",
      "JAVA_HOME: C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.28.6-hotspot\\\n",
      "SPARK_HOME: Not set\n",
      "PYTHONPATH: Not set\n",
      "âœ… Java is available\n",
      "openjdk version \"11.0.28\" 2025-07-15\n",
      "OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6)\n",
      "OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode)\n",
      "\n",
      "\n",
      "ðŸ” PySpark check:\n",
      "âœ… PySpark version: 3.5.1\n",
      "PySpark location: c:\\Users\\juraj\\Desktop\\ucoudify_pyspark\\pyspark_env_final\\Lib\\site-packages\\pyspark\\__init__.py\n",
      "\n",
      "ðŸ” Environment variables:\n",
      "JAVA_HOME: C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.28.6-hotspot\\\n",
      "SPARK_HOME: Not set\n",
      "PYTHONPATH: Not set\n"
     ]
    }
   ],
   "source": [
    "# Environment diagnostics\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ðŸ” Environment Diagnostics:\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if Java is available\n",
    "print(\"\\nðŸ” Java check:\")\n",
    "try:\n",
    "    import subprocess\n",
    "    result = subprocess.run(['java', '-version'], capture_output=True, text=True, shell=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"âœ… Java is available\")\n",
    "        print(result.stderr)\n",
    "    else:\n",
    "        print(\"âŒ Java not found in PATH\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error checking Java: {e}\")\n",
    "\n",
    "# Check PySpark installation\n",
    "print(\"\\nðŸ” PySpark check:\")\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"âœ… PySpark version: {pyspark.__version__}\")\n",
    "    print(f\"PySpark location: {pyspark.__file__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ PySpark import error: {e}\")\n",
    "\n",
    "print(\"\\nðŸ” Environment variables:\")\n",
    "for var in ['JAVA_HOME', 'SPARK_HOME', 'PYTHONPATH']:\n",
    "    value = os.environ.get(var, 'Not set')\n",
    "    print(f\"{var}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978b3d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark Session...\n",
      "Spark Session initialized successfully!\n",
      "Spark Version: 3.5.1\n",
      "Application Name: Account Balance Report\n",
      "Spark UI: http://BENDIKFILMSXPS:4040\n",
      "Spark Session initialized successfully!\n",
      "Spark Version: 3.5.1\n",
      "Application Name: Account Balance Report\n",
      "Spark UI: http://BENDIKFILMSXPS:4040\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session (this may take a minute on first run)\n",
    "\n",
    "print(\"Initializing Spark Session...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Account Balance Report\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session initialized successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e418df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Account Information loaded and date columns cast to DateType\n",
      "Schema:\n",
      "root\n",
      " |-- Account: integer (nullable = true)\n",
      " |-- Customer: string (nullable = true)\n",
      " |-- OpeningDate: date (nullable = true)\n",
      " |-- ClosingDate: date (nullable = true)\n",
      "\n",
      "Row count: 4\n",
      "+-------+--------+-----------+-----------+\n",
      "|Account|Customer|OpeningDate|ClosingDate|\n",
      "+-------+--------+-----------+-----------+\n",
      "|1000001|    John| 2024-11-30|       NULL|\n",
      "|1000002|   Betty| 2023-08-24|       NULL|\n",
      "|1000003| Jessica| 2024-05-14|       NULL|\n",
      "|1000004|    Josh| 2025-06-02| 2025-06-02|\n",
      "+-------+--------+-----------+-----------+\n",
      "\n",
      "+-------+--------+-----------+-----------+\n",
      "|Account|Customer|OpeningDate|ClosingDate|\n",
      "+-------+--------+-----------+-----------+\n",
      "|1000001|    John| 2024-11-30|       NULL|\n",
      "|1000002|   Betty| 2023-08-24|       NULL|\n",
      "|1000003| Jessica| 2024-05-14|       NULL|\n",
      "|1000004|    Josh| 2025-06-02| 2025-06-02|\n",
      "+-------+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Account Information (cast dates; treat 'NULL' as null)\n",
    "# Note: CSV files use semicolon (;) as separator and comma (,) as decimal separator\n",
    "from pyspark.sql import functions as F  # explicit alias for clarity and linting\n",
    "\n",
    "account_info_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"nullValue\", \"NULL\") \\\n",
    "    .csv(\"AccountInformation.csv\")\n",
    "\n",
    "# Cast date columns from dd/MM/yyyy to proper DateType\n",
    "account_info_df = account_info_df \\\n",
    "    .withColumn(\"OpeningDate\", F.to_date(F.col(\"OpeningDate\"), \"dd/MM/yyyy\")) \\\n",
    "    .withColumn(\"ClosingDate\", F.to_date(F.col(\"ClosingDate\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "print(\"Account Information loaded and date columns cast to DateType\")\n",
    "print(\"Schema:\")\n",
    "account_info_df.printSchema()\n",
    "print(f\"Row count: {account_info_df.count()}\")\n",
    "account_info_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10e003c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Loading AccountEntries.csv...\n",
      "--> Data loaded successfully. Starting preparation and cleaning...\n",
      "\n",
      "--> Preparation complete. Verifying the result:\n",
      "\n",
      "Final Schema:\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Account: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n",
      "\n",
      "Row count: 16\n",
      "\n",
      "Sample Data:\n",
      "--> Data loaded successfully. Starting preparation and cleaning...\n",
      "\n",
      "--> Preparation complete. Verifying the result:\n",
      "\n",
      "Final Schema:\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Account: integer (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- Text: string (nullable = true)\n",
      "\n",
      "\n",
      "Row count: 16\n",
      "\n",
      "Sample Data:\n",
      "+----------+-------+------------+--------+---------------+\n",
      "|      Date|Account|      Amount|Currency|           Text|\n",
      "+----------+-------+------------+--------+---------------+\n",
      "|2025-03-23|1000001|    -1243.34|     EUR|           Rent|\n",
      "|2025-03-12|1000002|      5434.4|     EUR|       Services|\n",
      "|2025-03-04|1000003|     -2323.4|     EUR|      Utilities|\n",
      "|2025-03-11|1000004|     -3422.9|     EUR|       Salaries|\n",
      "|2025-01-10|1000001|     23423.4|     EUR|    Invoice 343|\n",
      "|2025-01-17|1000002|      -312.4|     EUR|           Fees|\n",
      "|2025-01-28|1000003|    12123.33|     EUR|        Inv #88|\n",
      "|2025-01-30|1000004|     -2354.3|     EUR|   Travel costs|\n",
      "|2025-02-02|1000001|    276747.4|     EUR|   Monthly fees|\n",
      "|2025-02-13|1000002|       654.4|     EUR|Cost adjustment|\n",
      "|2025-02-17|1000003|     -5645.4|     EUR|      Utilities|\n",
      "|2025-02-28|1000004|    476455.6|     EUR|       Transfer|\n",
      "|2025-01-01|1000001|   3322909.9|     EUR|Opening balance|\n",
      "|2025-01-01|1000002|3.83839924E7|     EUR|Opening balance|\n",
      "|2025-01-01|1000003|   5584843.9|     EUR|Opening balance|\n",
      "|2025-01-01|1000004|    988786.9|     EUR|Opening balance|\n",
      "+----------+-------+------------+--------+---------------+\n",
      "\n",
      "+----------+-------+------------+--------+---------------+\n",
      "|      Date|Account|      Amount|Currency|           Text|\n",
      "+----------+-------+------------+--------+---------------+\n",
      "|2025-03-23|1000001|    -1243.34|     EUR|           Rent|\n",
      "|2025-03-12|1000002|      5434.4|     EUR|       Services|\n",
      "|2025-03-04|1000003|     -2323.4|     EUR|      Utilities|\n",
      "|2025-03-11|1000004|     -3422.9|     EUR|       Salaries|\n",
      "|2025-01-10|1000001|     23423.4|     EUR|    Invoice 343|\n",
      "|2025-01-17|1000002|      -312.4|     EUR|           Fees|\n",
      "|2025-01-28|1000003|    12123.33|     EUR|        Inv #88|\n",
      "|2025-01-30|1000004|     -2354.3|     EUR|   Travel costs|\n",
      "|2025-02-02|1000001|    276747.4|     EUR|   Monthly fees|\n",
      "|2025-02-13|1000002|       654.4|     EUR|Cost adjustment|\n",
      "|2025-02-17|1000003|     -5645.4|     EUR|      Utilities|\n",
      "|2025-02-28|1000004|    476455.6|     EUR|       Transfer|\n",
      "|2025-01-01|1000001|   3322909.9|     EUR|Opening balance|\n",
      "|2025-01-01|1000002|3.83839924E7|     EUR|Opening balance|\n",
      "|2025-01-01|1000003|   5584843.9|     EUR|Opening balance|\n",
      "|2025-01-01|1000004|    988786.9|     EUR|Opening balance|\n",
      "+----------+-------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Account Entries\n",
    "# Note: This data has European number format (comma as decimal separator)\n",
    "\n",
    "# --- Step 1: Load the AccountEntries.csv data from the file ---\n",
    "# We read all columns as text first (by not using inferSchema) because the 'Amount'\n",
    "# column has a complex format (with '.' as a thousands separator and ',' as a decimal separator)\n",
    "# that would cause automatic parsing to fail.\n",
    "print(\"--> Loading AccountEntries.csv...\")\n",
    "account_entries_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \";\") \\\n",
    "    .csv(\"AccountEntries.csv\")\n",
    "\n",
    "print(\"--> Data loaded successfully. Starting preparation and cleaning...\")\n",
    "\n",
    "# --- Step 2: Clean and transform the data into the correct types ---\n",
    "entries_prepared_df = account_entries_df \\\n",
    "    .withColumn(\n",
    "        \"Date\",\n",
    "        F.to_date(F.col(\"Date\"), \"dd/MM/yyyy\")  # Convert the Date string to a proper DateType\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"Amount\",\n",
    "        # First, remove the thousands separator ('.') -> '3.322.909,90' becomes '3322909,90'\n",
    "        F.regexp_replace(F.col(\"Amount\"), \"\\\\.\", \"\") \\\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"Amount\",\n",
    "        # Second, replace the decimal comma (',') with a decimal point ('.') -> '3322909,90' becomes '3322909.90'\n",
    "        F.regexp_replace(F.col(\"Amount\"), \",\", \".\") \\\n",
    "        .cast(DoubleType())  # Finally, cast the cleaned string to a numeric DoubleType\n",
    "    ) \\\n",
    "     .withColumn(\n",
    "        \"Account\", F.col(\"Account\").cast(\"integer\")\n",
    "    )\n",
    "    # .withColumnRenamed(\n",
    "    #     \"Account\", \"AccountNumber\" # Rename for clarity and easier joins later\n",
    "    # )\n",
    "   \n",
    "\n",
    "# --- Step 3: Verify the result ---\n",
    "print(\"\\n--> Preparation complete. Verifying the result:\")\n",
    "print(\"\\nFinal Schema:\")\n",
    "entries_prepared_df.printSchema()\n",
    "\n",
    "print(f\"\\nRow count: {entries_prepared_df.count()}\")\n",
    "print(\"\\nSample Data:\")\n",
    "entries_prepared_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
